---
tags:
  - emergence
  - attention
  - in-context-learning
status: ðŸŸ§ To Read
authors:
  - Nicolas Zucchet
  - Francesco D'Angelo
  - Andrew Lampinen
  - Stephanie Chan
conference: NeurIPS
year: 2025
link: https://openreview.net/forum?id=jMhRbV47pS
priority: â­â­â­â­
created: 2025-12-30
---

# The emergence of sparse attention - impact of data distribution and benefits of repetition

> [!abstract] Abstract
> Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.

---

## ðŸ”‘ Key Contributions
- 
- 
- 

## ðŸ“ Summary
> [!summary] TL;DR
>  Learning sparse attention is prone to emerging behaviors during training, and data and model design influence emergence speed.


### Background & Motivation
- What problem are they solving?

### Methodology
- How did they solve it?

### Experiments & Results
- What did they find?

---

## ðŸ’­ Critical Analysis / Thoughts
> [!quote] My Take
> Strengths, weaknesses, and potential future work.

- **Pros**:
- **Cons**:

---

## ðŸ”— References & Links
- [Link to Paper](URL)
- [Code Repository](URL)
