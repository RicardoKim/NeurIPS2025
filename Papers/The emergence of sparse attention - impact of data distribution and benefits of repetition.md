---
tags:
  - emergence
  - attention
  - in-context-learning
status: ðŸŸ§ To Read
authors:
  - Nicolas Zucchet
  - Francesco D'Angelo
  - Andrew Lampinen
  - Stephanie Chan
conference: NeurIPS
year: 2025
link: https://openreview.net/forum?id=jMhRbV47pS
priority: â­â­â­â­
created: 2025-12-30
---

# The emergence of sparse attention - impact of data distribution and benefits of repetition

> [!abstract] Abstract
> Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.

---

## ðŸ”‘ Key Contributions
- Identifies sparse attention learning as a mechanistic cause of emergent behavior
- Derives predictive scaling laws for emergence time as a function of data, model, and optimization parameters
- Demonstrates the causal role of repetition in accelerating emergence
- Connects emergence theory to induction heads and in-context learning

## ðŸ“ Summary
> [!summary] TL;DR
>  Emergence in Transformers can be explained and predicted by how and when attention becomes sparse; repetition in data dramatically speeds up this process.


### Background & Motivation
- Emergent behaviors (sharp capability jumps during training) are well-documented but mostly  lack predictive theory.
- Key question:  *Can the learning dynamics of sparse attention explain emergence, and can we control it via data distribution?*

### Methodology
theory -> synthetic validation -> Transformer -> in-context learning
**1. Simple Transformers Model**
- Gradient-flow analysis of a simplified attention-based model.
- Task: sequence regression where the target depends only on a single token position.
- Enables closed-form predictions of plateau length (emergence time).

**2. Synthetic Sparse-Attention Regression (Transformers)**
- Small Transformers trained on the same task.
**Repetition types**
- _In-context repetition_: the relevant token appears multiple times in the sequence
- _Cross-sample repetition_: certain tokens are overrepresented across the dataset.

**3. In-Context Associative Recall (Induction Head Task)**
- Keyâ€“value pairs followed by a query key.
- Success requires attending to the correct prior key (induction head).
- Tests whether sparse-attention emergence generalizes beyond toy regression.

**Metrics & Analysis**
- **Emergence time** measured as steps to cross a fixed performance threshold ("plateau length").
### Experiments & Results
- Training exhibits multi-phase dynamics: long plateau leads to sudden performance jump.
- Emergence time follows power laws:
    - Increases with sequence length and dimensionality.
    - Decreases with repetition.
- **Repetition strongly accelerates emergence**, even when test data has no repetition.
- Similar dynamics observed in **induction-head formation** for in-context learning.
- Excessive repetition can cause overfitting**, but longer training mitigates this.

---

## ðŸ’­ Critical Analysis / Thoughts
> [!quote] My Take
> The paperâ€™s major strength is its clean mechanistic explanation and predictive control of emergence via sparse attention and repetition, but its reliance on synthetic tasks and nonstandard architectures limits immediate generalization to real LLMs, making future work on validating these scaling laws under natural language data, LayerNorm-equipped models, and large-scale pretraining essential.

- **Pros**:
	- Provides an explanation for emergence, moving beyond just scaling.
	- Shows that emergence timing can be estimated *before training*.
	- Theoretically links learning dynamics with data distribution and interpretability

- **Cons**:
	- Heavy reliance on synthetic tasks limits direct expansion to natural language tasks
	- I think repetition benefits are task-dependent and can trade off with generalization.
	- The authors remove layer normalization, so actual LLM behavior may be different
---
