---
tags:
  - paper
  - research
status: ðŸŸ© Done
authors:
  - Mintong Kang,Â 
  - Zhaorun Chen
  - Bo Li
conference: NeurIPS
year: 2025
link: https://openreview.net/forum?id=nOsEyBGk1I
priority: â­â­â­â­
created: 2026-01-12
---

# C-SafeGen Certified Safe LLM Generation with Claim-Based Streaming Guardrails

> [!abstract] Abstract
> Despite the remarkable capabilities of large language models (LLMs) across diverse applications, they remain vulnerable to generating content that violates safety regulations and policies. To mitigate these risks, LLMs undergo safety alignment; however, they can still be effectively jailbroken. Off-the-shelf guardrail models are commonly deployed to monitor generations, but these models primarily focus on detection rather than ensuring safe decoding of LLM outputs. Moreover, existing efforts lack rigorous safety guarantees, which are crucial for the universal deployment of LLMs and certifiable compliance with regulatory standards. In this paper, we propose a Claim-based Stream Decoding (CSD) algorithm coupled with a statistical risk guarantee framework using conformal analysis. Specifically, our CSD algorithm integrates a stream guardrail model to safeguard sequential claims generated by LLMs and incorporates a backtracking mechanism to revise claims flagged with high safety risks. We provide theoretical guarantees demonstrating that the CSD algorithm achieves the desired generation distribution subject to safety constraints. Furthermore, we introduce a generation risk certification framework and derive a high-probability upper bound on the safety risk of the proposed CSD algorithm. We extend our approach to online settings, where user queries arrive sequentially, and prove that our method can asymptotically control safety risk to any desired level. Empirical evaluations demonstrate the effectiveness and efficiency of the CSD algorithm compared to state-of-the-art safety decoding approaches. Additionally, we validate the soundness and tightness of the derived safety risk upper bound using realistic data in both offline and online scenarios.

---

## ðŸ”‘ Key Contributions
- Created the CSD algorithm
- Provided theoretical proofs that the claim based stream decoding method provides quantifiable guarantees that a certain LLM generated claim is safe.
- Provides empirical evidence and substantial proof that safety risk can be customized to any desired level, demonstrating effectiveness and efficiency of the CSD algorithm compared to SOTA safety. 

## ðŸ“ Summary
> [!summary] TL;DR
> Assume all LLMs generate "claims" sequentially. We can use guardrail models (like llama-guard) to generate safety risk probability (0-1) for every "claim" sequentially generated by an LLM. If a "claim" is high risk, backtrack to the last safe claim and re-generate the next claim.

### Background & Motivation
- Safety alignment and guardrails reduce harmful generations, but do not provide decoding time guarantees. 
	- Still remain vulnerable to jailbreaks
- C-SafeGen provides high-probabiity bound on safety risk

### Methodology
- Theory: we can use *conformal risk control* to certify a high-probability upper bound on the expected safety risk for a fixed generation
	- Basically, for each statement (e.g. "Yogurt is dumb") we can assign a value of the safety risk (~ 0.2 safe)
- Assume LLMs generate by "claim" or statement
	1. Yogurt is a dairy product.
	2. Everybody loves to eat yogurt
	3. Black people hate yogurt because its white (misinfomation: harmful probability = 0.9)
- If unsafe, backtrack (in the above case, go back to 2)
### Experiments & Results
- Good theoretical evidence
- Good empirical evidence

---

## ðŸ’­ Critical Analysis / Thoughts
> [!quote] My Take
> Scaling base model does not necessarily improve safety. Also, best-of-n sampling is much slower than decoding

- **Pros**: Maybe this can extend to hallucinations? If we have a judge model in the middle of an agentic process, we can have a judge determine if something hallucinates, and preemptively backtrack for better generation.
- **Cons**: Current method is overly reliant on the probability output of the guardrail (kind of limited.)

---

## ðŸ”— References & Links
- [Link to Paper](https://openreview.net/forum?id=nOsEyBGk1I)
- [Code Repository](URL)
